import "./index.css";
import { Component } from "react";
import ProjectSlider from "./ProjectSlider";
const projectList = [
  {
    id: 1,
    imageUrl:
      "https://res.cloudinary.com/dxlyuzv7w/image/upload/v1699502145/p1_lhnvcn.png",
    heading:
      "Design and Development of Autonomous Underwater Vehicle: Ocean Voyager",
    description:
      "Focused on creating smart algorithms for AUVs, enabling autonomous underwater vision-guided navigation. Worked on SIFT algorithm (Difference of Gaussians and Space Generation) to describe local features under water.",
    skillsUsed:
      "Underwater robotics, Sensor integration, Control systems, System integration",
    more:
      "Autonomous underwater vehicles (AUVs) are submerged vehicles that are operated by onboard computers. AUV formation refers to a collaborative control strategy that aims to coordinate the movement of several AUVs in a cohesive manner while carrying out certain duties. Multi-AUV formations offer superior efficiency and enhanced stability compared to individual AUVs. This is particularly advantageous in several fields, including oil and gas companies, hydrographic surveys, and military operations. In order to improve formation, there are various crucial variables, such as AUV performance, formation control, and communication capabilities. However, the majority of studies in the field of autonomous underwater vehicle (AUV) formation primarily concentrate on strategies for controlling the formation. It is evident that the study on the communication capacity and performance of various Autonomous Underwater Vehicle (AUV) formations is still in its first phase. Researchers derive advantages by providing an all-encompassing overview of the current status of AUV formation research and development. This work provides a comprehensive analysis of AUVs, focusing specifically on formation control and the underwater acoustic communication capacity. We present a categorization system consisting of three dimensions: AUV performance, formation control, and communication capacity. This framework offers a thorough categorization approach for future study on Autonomous Underwater Vehicle (AUV) formations. Additionally, it may be utilised to evaluate and contrast distinct techniques, aiding engineers in selecting appropriate forming procedures for diverse applications. In addition, our review examines the design of formation architecture considering limitations in communication. We also highlight certain myths and problematic studies in the field of formation control that are connected to communication.",
  },
  {
    id: 2,
    imageUrl:
      "https://res.cloudinary.com/dp4e9sqxj/image/upload/v1699596396/p2-1_vuepux.png",
    heading: "Water to Cloud: Realtime River Monitoring System",
    description:
      "Encompasses setting up sensors in rivers, gathering data like water quality, temp., pH, and revealing correlations. Used fog computing to establish a seamless and efficient system, ensuring data reliability and timely insights.",
    skillsUsed:
      "Cloud computing, Remote monitoring, Real-time visualization, GIS Mapping, System scalability.",
    more:
      "Water to Cloud: Realtime River Monitoring System is an innovative project designed to revolutionize the monitoring and management of river ecosystems. The system employs advanced sensor technologies, data analytics, and cloud computing to provide real-time insights into the health and dynamics of rivers. At its core, the project involves the deployment of a network of smart sensors strategically placed along riverbanks. These sensors are equipped with a range of environmental monitoring capabilities, including water quality analysis, temperature sensing, and flow rate measurements. The collected data is then transmitted in real-time to a cloud-based platform for comprehensive analysis and visualization. The key technical components of the Realtime River Monitoring System include state-of-the-art sensors capable of capturing a variety of parameters crucial to understanding river health. These sensors utilize cutting-edge technologies such as spectroscopy and chemical analysis to assess water quality, ensuring accurate and reliable data collection. Additionally, the system incorporates advanced weather sensors to monitor atmospheric conditions, providing a holistic view of the factors influencing river ecosystems. The real-time data collected by the sensors is transmitted securely to a cloud-based infrastructure. Here, the data undergoes sophisticated analytics processes, leveraging machine learning algorithms to detect patterns, anomalies, and trends. This analytical layer enables stakeholders, including environmental agencies and researchers, to make informed decisions based on up-to-the-minute information about the river's condition. Furthermore, the project emphasizes the use of user-friendly interfaces and dashboards accessible via web and mobile applications. This ensures that stakeholders can easily interpret the complex data generated by the system, fostering widespread adoption and understanding. In summary, Water to Cloud: Realtime River Monitoring System represents a pioneering effort to fuse advanced sensor technologies with cloud computing for the real-time monitoring of rivers. By providing actionable insights into water quality, temperature, and flow dynamics, the system aims to enhance the conservation and sustainable management of river ecosystems.",
  },
  {
    id: 3,
    imageUrl:
      "https://res.cloudinary.com/dp4e9sqxj/image/upload/v1699595349/p3_tqrlpw.png",
    heading:
      "AI-enhanced Breast Health Assessment: Advancing Early Detection and Care",
    description:
      "DL model that empowers timely breast health detection, and monitoring with Hadamard approach and LSTMs. Developed circularly polarized wearable sensor for monitoring breast health all the time and monitoring women.",
    skillsUsed: null,
    more:
      "AI-enhanced Breast Health Assessment: Advancing Early Detection and Care. The AI-enhanced Breast Health Assessment project represents a groundbreaking initiative aimed at revolutionizing the early detection and continuous monitoring of breast health using cutting-edge technologies. At its core, the project leverages a Deep Learning (DL) model, integrating the Hadamard approach and Long Short-Term Memory (LSTM) networks to empower timely breast health detection and ongoing monitoring. The DL model serves as the intelligence behind the system, capable of processing complex datasets and identifying patterns indicative of breast health issues. The Hadamard approach, a mathematical technique, enhances the model's efficiency in processing and extracting meaningful information from large-scale data. This combination of DL and Hadamard offers a robust solution for accurate and efficient breast health assessment. Complementing the DL model is the incorporation of LSTM networks, a type of recurrent neural network designed for processing sequential data. In the context of breast health monitoring, LSTM networks play a pivotal role in capturing temporal dependencies within data, allowing the system to understand and predict changes in breast health over time. This temporal aspect is crucial for identifying subtle variations and ensuring early detection of anomalies. To facilitate continuous monitoring, the project introduces a circularly polarized wearable sensor designed specifically for monitoring breast health around the clock. The wearable sensor represents a significant advancement in health technology, offering a non-invasive and unobtrusive means of collecting real-time data. By seamlessly integrating into daily life, the wearable sensor ensures constant monitoring, providing a wealth of information for the DL model to analyze. One notable feature of the project is its focus on inclusivity and personalized care for women. The circularly polarized wearable sensor is tailored to monitor women's unique physiological characteristics, ensuring that the system adapts to individual variations and provides personalized insights. This approach not only enhances the accuracy of breast health assessments but also contributes to a more patient-centric healthcare experience. The implications of the AI-enhanced Breast Health Assessment project extend beyond early detection, reaching into the realm of preventive care and proactive health management. By combining advanced DL techniques, the Hadamard approach, LSTM networks, and wearable sensor technology, the project sets a new standard for breast health monitoring, empowering women with timely information and facilitating a paradigm shift towards proactive and personalized healthcare practices. As the project continues to evolve, it holds the promise of significantly improving outcomes in breast health, ultimately contributing to a healthier and more informed population.",
  },
  {
    id: 4,
    imageUrl:
      "https://res.cloudinary.com/dp4e9sqxj/image/upload/v1699595702/p4_isgji3.png",
    heading: "Frontier Forge: Dynamic 3D Scene Generation in Virtual Reality",
    description:
      "Created immersive VR environments through AI and generating dynamic 3D scenes enhancing VR interactions. Incorporating 3D Artificial Intelligence models with AR and VR sets with motion-based audio and music generation.",
    skillsUsed: null,
    more:
      "Frontier Forge: Dynamic 3D Scene Generation in Virtual Reality: The Frontier Forge project stands at the forefront of innovation, introducing a groundbreaking approach to virtual reality (VR) by seamlessly combining artificial intelligence (AI) with the creation of dynamic 3D scenes. This ambitious endeavor is designed to enhance the immersive nature of VR environments, providing users with a truly interactive and evolving experience. At the heart of Frontier Forge is the development of a sophisticated AI system capable of dynamically generating 3D scenes in real-time. This goes beyond traditional static VR environments, introducing an element of unpredictability and responsiveness. The AI's ability to adapt and generate scenes on the fly not only elevates the immersive quality of VR experiences but also ensures that users encounter fresh and engaging content with every interaction. One of the project's key features is the integration of 3D Artificial Intelligence models, enhancing the depth and complexity of the virtual environments. These models are trained to understand user behavior, preferences, and interactions within the VR space. As users navigate through the virtual world, the AI continuously analyzes their movements and responses, dynamically adjusting the 3D scenes to create a tailored and personalized experience. To further augment the sensory experience, Frontier Forge incorporates augmented reality (AR) and VR sets equipped with motion-based audio and music generation. This integration adds a new dimension to the virtual experience, synchronizing audiovisual elements with user movements. The result is a multisensory adventure where users not only see and interact with dynamic 3D scenes but also hear music and sound effects that respond to their actions, creating a truly immersive and responsive virtual reality. The implications of Frontier Forge extend beyond entertainment, finding applications in education, training, and various industries. The dynamic 3D scene generation capability can be harnessed for realistic simulations, training scenarios, and educational experiences, providing users with hands-on, engaging content that adapts to their actions and decisions. Moreover, the project contributes to the evolution of VR as a medium for artistic expression and storytelling. By combining AI-driven dynamic scene generation with motion-based audio and music, Frontier Forge opens new avenues for creators to craft interactive narratives and experiences that respond to the audience's presence and engagement. In summary, Frontier Forge represents a significant leap forward in the realm of virtual reality by introducing dynamic 3D scene generation powered by AI. The project's integration of 3D Artificial Intelligence models and motion-based audio and music enhances the immersive quality of VR experiences, offering users a personalized and evolving journey through virtual worlds. As technology continues to advance, Frontier Forge paves the way for the next generation of interactive and responsive virtual reality applications.",
  },
  {
    id: 5,
    imageUrl:
      "https://res.cloudinary.com/dp4e9sqxj/image/upload/v1699595959/p5_hxpijq.png",
    heading: "Contactless Heart Rate monitoring using Hartley Transform and AI",
    description:
      "Applied Hartley Transform for remote heart rate monitoring, for analyzing cardiovascular and systolic rhythms. Utilised Eulerian Video method and GoogleNet architecture to fabricate accurate non-inavsive heart rate sensor.",
    skillsUsed: null,
    more:
      "Contactless Heart Rate Monitoring using Hartley Transform and AI: The project on Contactless Heart Rate Monitoring represents a pioneering effort in the realm of health technology, combining advanced signal processing techniques with artificial intelligence (AI) to enable remote and non-invasive monitoring of heart rates. The project primarily leverages the Hartley Transform for precise analysis of cardiovascular and systolic rhythms, and integrates the Eulerian Video method along with the GoogleNet architecture to develop an accurate and non-invasive heart rate sensor. The application of the Hartley Transform is a key aspect of this project, providing a powerful tool for the remote monitoring of heart rates. Unlike traditional methods, the Hartley Transform operates in the frequency domain, allowing for a more direct analysis of signals related to cardiovascular and systolic rhythms. This transformative approach enhances the accuracy of heart rate monitoring and provides valuable insights into the overall cardiovascular health of individuals. To implement the contactless heart rate monitoring system, the project employs the Eulerian Video method. This method involves the analysis of subtle color changes in facial regions, typically around the forehead or cheek, which are indicative of blood flow changes associated with the cardiac cycle. By focusing on these color variations, the system can accurately extract the heart rate information without requiring any physical contact with the individual. In conjunction with the Eulerian Video method, the project utilizes the GoogleNet architecture, a deep neural network renowned for its proficiency in image recognition tasks. The integration of GoogleNet enhances the precision of the non-invasive heart rate sensor by efficiently extracting and processing facial features from the video feed. This combination of advanced AI architecture and signal processing techniques ensures a reliable and robust system for contactless heart rate monitoring. The project's significance lies in its potential applications across various domains, ranging from healthcare and fitness to remote patient monitoring. The non-invasive nature of the heart rate sensor makes it particularly valuable for scenarios where traditional contact-based sensors may be impractical or uncomfortable. Additionally, the integration of AI enhances the adaptability of the system, allowing it to adapt to individual variations in facial features and environmental conditions. In conclusion, the Contactless Heart Rate Monitoring project stands as a testament to the convergence of signal processing and artificial intelligence in the realm of healthcare technology. By leveraging the Hartley Transform for frequency domain analysis and combining it with the Eulerian Video method and GoogleNet architecture, the project pioneers a sophisticated yet non-invasive approach to heart rate monitoring. This innovation holds the potential to revolutionize how we monitor cardiovascular health, providing a convenient and accurate solution for remote heart rate assessment.",
  },
];

class Projects extends Component {
  render() {
    return (
      <div className="project-card">
        <h1>Projects</h1>
        <br />
        <ProjectSlider projectsList={projectList} />
      </div>
    );
  }
}

export default Projects;
